{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82f2ab3",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95642ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daff76",
   "metadata": {},
   "source": [
    "# Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c363a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "\n",
    "# root folder\n",
    "data_root_dpath = os.path.join(\"..\", \"..\", \"data\", \"OPP-115\")\n",
    "\n",
    "# annotations folder\n",
    "annot_dpath = os.path.join(data_root_dpath, \"annotations\")\n",
    "\n",
    "# policy collection metadata file\n",
    "policy_collect_metadata_fpath = os.path.join(data_root_dpath, \"documentation\", \"policies_opp115.csv\")\n",
    "\n",
    "# Website metadata file\n",
    "site_metadata_fpath = os.path.join(data_root_dpath, \"documentation\", \"websites_opp115.csv\")\n",
    "\n",
    "# Sanitized policies folder\n",
    "sanitized_pol_dpath = os.path.join(data_root_dpath, \"sanitized_policies\")\n",
    "\n",
    "# OUTPUT\n",
    "\n",
    "# processed data folder\n",
    "processed_data_dpath = os.path.join(data_root_dpath, \"processed_data\")\n",
    "\n",
    "# processed annotation folder\n",
    "op_annotations_dpath = os.path.join(processed_data_dpath, \"processed_annotations\")\n",
    "\n",
    "# processed segments folder\n",
    "op_segments_dpath = os.path.join(processed_data_dpath, \"processed_segments\")\n",
    "\n",
    "# master annotations file\n",
    "master_annotations_115_fpath = os.path.join(processed_data_dpath, \"master_annotations_115.csv\")\n",
    "\n",
    "# categy-wise split annotations folder (w/o parsed JSON attr)\n",
    "catsplit_annotations_115_unparsed_dpath = os.path.join(processed_data_dpath, \"catsplit_annotations_115_unparsed\")\n",
    "\n",
    "# categy-wise split annotations folder (w/ parsed JSON attr)\n",
    "catsplit_annotations_115_parsed_dpath = os.path.join(processed_data_dpath, \"catsplit_annotations_115_parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a42fc0",
   "metadata": {},
   "source": [
    "# Process annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270542b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in glob.glob(r\"{}/*.csv\".format(annot_dpath)):   \n",
    "    \n",
    "    #Extract path basename\n",
    "    basename = os.path.basename(fname)\n",
    "\n",
    "    #Create directories if they don't exist\n",
    "    os.makedirs(op_annotations_dpath, exist_ok = True)\n",
    "    os.makedirs(op_segments_dpath, exist_ok = True)\n",
    "    \n",
    "    #Extract policyID from basename\n",
    "    policy_id = basename.split('_')[0]\n",
    "    policy_df = pd.read_csv(fname, header=None, usecols=[0, 4, 5, 6], names=['annotation_ID', 'segment_ID', 'category', 'attr_val'])\n",
    "    \n",
    "    #Set policyID in each table\n",
    "    policy_df.loc[:,\"policy_ID\"] = policy_id\n",
    "    \n",
    "    #Replace extension\n",
    "    santized_policy_fpath = os.path.splitext(basename)[0]+'.html'\n",
    "    \n",
    "    # Parse html text\n",
    "    html = open(os.path.join(sanitized_pol_dpath, santized_policy_fpath), \"r\").read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    soup_text = soup.get_text()\n",
    "    \n",
    "    #Match segments with their segment IDs for each policy\n",
    "    segments = soup_text.split(\"|||\")\n",
    "    segments_df = pd.DataFrame(segments, columns = [\"segment_text\"])\n",
    "    segments_df.index.name = \"segment_ID\"\n",
    "    segments_df.reset_index(inplace = True)\n",
    "    \n",
    "    #Save processed segments\n",
    "    segments_df.to_csv(os.path.join(op_segments_dpath, basename), index = False)\n",
    "    \n",
    "    policy_df_merged = policy_df.merge(segments_df, on='segment_ID', how = \"inner\")\n",
    "    \n",
    "    #Save processed policies\n",
    "    policy_df_merged.to_csv(os.path.join(op_annotations_dpath, basename), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36311d",
   "metadata": {},
   "source": [
    "# Merge all site-wise annotation into a master dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04560014",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for fname in glob.glob(r\"{}/*.csv\".format(op_annotations_dpath)):\n",
    "    policy_df = pd.read_csv(fname)\n",
    "    df_list.append(policy_df)\n",
    "\n",
    "master_annotations_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "master_annotations_df.to_csv(os.path.join(master_annotations_115_fpath), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a3ce8",
   "metadata": {},
   "source": [
    "# Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "assert(master_annotations_df.isnull().any(axis=1).any() == False)\n",
    "#missing_data_rows = master_annotations_df.index[master_annotations_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292189f",
   "metadata": {},
   "source": [
    "# Split the master dataframe wrt the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ce9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby category and split\n",
    "cat_dfs_list = [df for _, df in master_annotations_df.groupby('category')]\n",
    "\n",
    "#Directory exist check\n",
    "os.makedirs(catsplit_annotations_115_unparsed_dpath, exist_ok = True)\n",
    "\n",
    "#Save them into corresponding .CSV files\n",
    "for df in cat_dfs_list:\n",
    "    assert(len(set(df.category)) == 1)\n",
    "    category = '_'.join(next(iter(set(df.category))).replace(\"/\", \"-\").split(\" \"))\n",
    "    df.to_csv(os.path.join(catsplit_annotations_115_unparsed_dpath, \"{}.csv\".format(category)), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724c694",
   "metadata": {},
   "source": [
    "# Parse Attribute-value JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e8156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in glob.glob(r\"{}/*.csv\".format(catsplit_annotations_115_unparsed_dpath)):   \n",
    "    policy_df = pd.read_csv(fname)\n",
    "    assert(len(set(policy_df.category)) == 1)\n",
    "    category = '_'.join(next(iter(set(policy_df.category))).replace(\"/\", \"-\").split(\" \"))\n",
    "    os.makedirs(catsplit_annotations_115_parsed_dpath, exist_ok = True)\n",
    "\n",
    "    cat_list_of_dict = []\n",
    "    for index, row in policy_df.iterrows():\n",
    "        attr_dict = json.loads(row[\"attr_val\"])\n",
    "        cat_list_of_dict.append({ k:v['value'] for k,v in attr_dict.items() })\n",
    "    cat_df = pd.DataFrame(cat_list_of_dict)\n",
    "    assert(cat_df.isnull().any(axis=1).any() == False)\n",
    "    pd.concat((policy_df, cat_df), axis = 1).to_csv(os.path.join(catsplit_annotations_115_parsed_dpath, \"{}.csv\".format(category)), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cb31c",
   "metadata": {},
   "source": [
    "# Pre-process site metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740017c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65705cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_metadata_df = pd.read_csv(site_metadata_fpath)\n",
    "# manually added a us rank of 0 to a missing value for policy UID 745 \n",
    "\n",
    "alexa_rank_global = []\n",
    "alexa_rank_us = []\n",
    "sectors = []\n",
    "\n",
    "for index, row in site_metadata_df.iterrows():\n",
    "    sector_lst = []\n",
    "    alexa_rank_global.append(re.findall(r'\\d+', row[\"Comments\"])[0])\n",
    "    alexa_rank_us.append(re.findall(r'\\d+', row[\"Comments\"])[1])\n",
    "            \n",
    "    sector_lst = list(set([row.iloc[i].split(\":\")[0] for i in range(7, site_metadata_df.shape[1]) if not row.iloc[i] != row.iloc[i]]))\n",
    "    sectors.append(sector_lst)\n",
    "\n",
    "\n",
    "pd.DataFrame({ 'site_name': site_metadata_df[\"Site Human-Readable Name\"].values,\n",
    "              'policy_ID': site_metadata_df[\"Policy UID\"].values,\n",
    "              'alexa_rank_global': alexa_rank_global,\n",
    "              'alexa_rank_us': alexa_rank_us,\n",
    "              'sectors': sectors\n",
    "}).to_csv(os.path.join(processed_data_dpath, \"site_metadata_115.csv\"), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf5a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-env",
   "language": "python",
   "name": "master-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}